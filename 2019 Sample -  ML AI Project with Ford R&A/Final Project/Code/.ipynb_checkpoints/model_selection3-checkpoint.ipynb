{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Created by Luis Alejandro (alejand@umich.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Model selection is choosing between different complexity or flexibility to fit/explain your data.\n",
    "#### Complexity Controlling Parameters\n",
    "* Not used to fit the data\n",
    "* Usually called \"hyperparameters\" (i.e., in Bayesian models parameters of the prior for instance)\n",
    "\n",
    "#### Selecting best complexity parameters\n",
    "Selecting the best complexity parameters usually consist of exploring different combinations of the hyperparameters to explain your data. More technically this search consist of:\n",
    "* An estimator (regressor or classifier )\n",
    "* A parameter space\n",
    "* A method for searching or sampling candidates\n",
    "* A cross-validation scheme\n",
    "* A score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.reports as rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1\n",
    "Performs model selection of the following hyperparameters applied to the bank dataset (customers leaving):\n",
    "* Network architecture\n",
    "* Alpha (Regularization)\n",
    "\n",
    "This is perform using cross-validation and a grid search using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataset from file\n",
    "dataset = pd.read_csv('../../datasets/classification/bank_exiting.csv')\n",
    "predictors = dataset.iloc[:,3:-1].values\n",
    "responses = dataset.iloc[:,-1].values\n",
    "# Encoding categorical data\n",
    "encoder_x1 = LabelEncoder()\n",
    "predictors[:,1] = encoder_x1.fit_transform(predictors[:,1]) # only 0 or 1 after this (just one column needed)\n",
    "encoder_x2 = LabelEncoder()\n",
    "predictors[:,2] = encoder_x2.fit_transform(predictors[:,2]) # more than two categories (use onehotencoder)\n",
    "ct = ColumnTransformer([('country_category', OneHotEncoder(categories='auto'),[1])], remainder='passthrough')\n",
    "predictors = ct.fit_transform(predictors)\n",
    "predictors = predictors[:,1:]\n",
    "X,X_holdout,y,y_holdout = train_test_split(predictors, responses, test_size = 0.2, random_state = 0)\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_holdout = sc.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  9.5934167  seconds\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.862 (std: 0.004)\n",
      "Parameters: {'alpha': 2, 'hidden_layer_sizes': (20, 6)}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.862 (std: 0.004)\n",
      "Parameters: {'alpha': 0, 'hidden_layer_sizes': (6,)}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.862 (std: 0.003)\n",
      "Parameters: {'alpha': 0.8, 'hidden_layer_sizes': (6,)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performs grid search\n",
    "start = time.perf_counter()\n",
    "architecture_choices = [(20, 6), (6), (20)]\n",
    "alpha_choices = [0,0.1,0.3,0.8,1,2,10]\n",
    "\n",
    "hyperparams = [{\n",
    "    'hidden_layer_sizes': architecture_choices,\n",
    "    'alpha': alpha_choices\n",
    "}]\n",
    "\n",
    "mdl = MLPClassifier(solver = 'lbfgs', activation='logistic')\n",
    "validator = GridSearchCV(mdl, cv=3, param_grid=hyperparams, scoring='accuracy', n_jobs=-1,verbose = 1)\n",
    "validator.fit(X,y)\n",
    "end = time.perf_counter()\n",
    "print('Elapsed time: ', end - start, ' seconds\\n')\n",
    "rp.report_grid_search(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (Metrics): \n",
      "\n",
      "Accuracy:  0.86\n",
      "F1 Score:  0.61\n",
      "Recall:  0.73\n",
      "Precision:  0.53\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation in the holdout set\n",
    "y_pred = validator.predict(X_holdout)\n",
    "rp.report_classification(y_pred, y_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2\n",
    "Performs model selection of the following hyperparameters applied to the wine dataset:\n",
    "* Network architecture\n",
    "* Alpha (Regularization)\n",
    "* Activation function\n",
    "\n",
    "This is perform using cross-validation and a grid search using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Notice that no standarization is applied in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.load_wine()\n",
    "predictors = dataset.data\n",
    "responses = dataset.target\n",
    "X,X_holdout,y,y_holdout = train_test_split(predictors, responses, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  3.1498118999999996  seconds\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.851 (std: 0.081)\n",
      "Parameters: {'activation': 'logistic', 'alpha': 0.8, 'hidden_layer_sizes': (20,)}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.803 (std: 0.012)\n",
      "Parameters: {'activation': 'logistic', 'alpha': 0.8, 'hidden_layer_sizes': (100,)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.795 (std: 0.092)\n",
      "Parameters: {'activation': 'logistic', 'alpha': 2, 'hidden_layer_sizes': (100,)}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:    3.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Performs grid search\n",
    "start = time.perf_counter()\n",
    "architecture_choices = [(100,20), (100,), (20,)]\n",
    "alpha_choices = [0,0.1,0.3,0.8,1,2,10]\n",
    "activation_choices = ['logistic', 'relu']\n",
    "\n",
    "hyperparams = [{\n",
    "    'hidden_layer_sizes': architecture_choices,\n",
    "    'alpha': alpha_choices,\n",
    "    'activation': activation_choices\n",
    "}]\n",
    "\n",
    "mdl = MLPClassifier(solver = 'lbfgs', max_iter = 200)\n",
    "validator = GridSearchCV(mdl, cv=5, param_grid=hyperparams, scoring='accuracy', n_jobs=-1,verbose = 1,iid = False)\n",
    "validator.fit(X,y)\n",
    "end = time.perf_counter()\n",
    "print('Elapsed time: ', end - start, ' seconds\\n')\n",
    "rp.report_grid_search(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (Metrics): \n",
      "\n",
      "Accuracy:  0.72\n",
      "F1 Score:  0.59\n",
      "Recall:  0.54\n",
      "Precision:  0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wichi\\Anaconda3\\envs\\machine_learning\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\wichi\\Anaconda3\\envs\\machine_learning\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation in the holdout set\n",
    "y_pred = validator.predict(X_holdout)\n",
    "rp.report_classification(y_pred, y_holdout,avg='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3\n",
    "Performs model selection of the following hyperparameters applied to the wine dataset:\n",
    "* Network architecture\n",
    "* Alpha (Regularization)\n",
    "* Activation function\n",
    "\n",
    "This is perform using cross-validation and a grid search using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Since we want to apply standarization, we must use a [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) to correctly standarize and evaluate the models during the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.load_wine()\n",
    "predictors = dataset.data\n",
    "responses = dataset.target\n",
    "X,X_holdout,y,y_holdout = train_test_split(predictors, responses, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "Elapsed time:  3.4068946999999987  seconds\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.979 (std: 0.028)\n",
      "Parameters: {'classifier__activation': 'logistic', 'classifier__alpha': 10, 'classifier__hidden_layer_sizes': (100,)}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.979 (std: 0.028)\n",
      "Parameters: {'classifier__activation': 'logistic', 'classifier__alpha': 10, 'classifier__hidden_layer_sizes': (20,)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.972 (std: 0.042)\n",
      "Parameters: {'classifier__activation': 'logistic', 'classifier__alpha': 2, 'classifier__hidden_layer_sizes': (100, 20)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.972 (std: 0.042)\n",
      "Parameters: {'classifier__activation': 'logistic', 'classifier__alpha': 2, 'classifier__hidden_layer_sizes': (100,)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.972 (std: 0.042)\n",
      "Parameters: {'classifier__activation': 'logistic', 'classifier__alpha': 2, 'classifier__hidden_layer_sizes': (20,)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.972 (std: 0.042)\n",
      "Parameters: {'classifier__activation': 'relu', 'classifier__alpha': 10, 'classifier__hidden_layer_sizes': (100,)}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.972 (std: 0.042)\n",
      "Parameters: {'classifier__activation': 'relu', 'classifier__alpha': 10, 'classifier__hidden_layer_sizes': (20,)}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Performs grid search\n",
    "start = time.perf_counter()\n",
    "\n",
    "sc = StandardScaler()\n",
    "clf = MLPClassifier(solver = 'lbfgs', max_iter = 200)\n",
    "estimators = [('normalizer', sc), ('classifier', clf)]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "architecture_choices = [(100,20), (100,), (20,)]\n",
    "alpha_choices = [0,0.1,0.3,0.8,1,2,10]\n",
    "activation_choices = ['logistic', 'relu']\n",
    "\n",
    "hyperparams = [{\n",
    "    'classifier__hidden_layer_sizes': architecture_choices,\n",
    "    'classifier__alpha': alpha_choices,\n",
    "    'classifier__activation': activation_choices\n",
    "}]\n",
    "\n",
    "validator = GridSearchCV(pipe, cv=5, param_grid=hyperparams, scoring='accuracy', n_jobs=-1,verbose = 1,iid = False)\n",
    "validator.fit(X,y)\n",
    "end = time.perf_counter()\n",
    "print('Elapsed time: ', end - start, ' seconds\\n')\n",
    "rp.report_grid_search(validator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (Metrics): \n",
      "\n",
      "Accuracy:  1.00\n",
      "F1 Score:  1.00\n",
      "Recall:  1.00\n",
      "Precision:  1.00\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation in the holdout set\n",
    "y_pred = validator.predict(X_holdout)\n",
    "rp.report_classification(y_pred, y_holdout,avg='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
